{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "In this notebook we perfomed the web scraping srcipts to obtain additional information based on the DOI tracker downloaded within the Excel file provided by the Web of Science search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from googlesearch import search\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Bibliometrix-Export-File-2021-11-17.xlsx', sheet_name='Sheet1')\n",
    "selected_columns = ['DI', 'TI']\n",
    "raw_data_copy = raw_data[selected_columns].copy()\n",
    "print ((raw_data_copy['DI'].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is to open the 'Bibliometrix-Export-File-2021-11-17.xlsx' and select only the DOI and Title columns (DI and TI respectively). Then we perform a Google search using the googlesearch library and search modulus, selecting only the first result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n"
     ]
    }
   ],
   "source": [
    "## Google search routine\n",
    "url_array = [] # auxiliar array to temporaly store the url\n",
    "## In this loop we go throug all articles passing the Google search query as the DOI code. If there is no DOI for the\n",
    "# specific article we pass an empty string\n",
    "for x in range (len(url_array), raw_data_copy['DI'].size):\n",
    "    query = raw_data_copy['DI'].iloc[x]\n",
    "    if pd.isna(query):\n",
    "        url_array.append('')\n",
    "        print (x)\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        for aux in search(query, tld=\"com\", num=1, stop=1, pause=2):\n",
    "            result = requests.get(aux)\n",
    "            assert result.status_code==200\n",
    "            url_array.append(aux)\n",
    "            print (x)\n",
    "            time.sleep(60)\n",
    "### It is important to set a waiting time in between the searches because if you perform to much searches te server will\n",
    "# cut down the communication. We need to work on the best value to wait to optimize our search procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['URL'] = url_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the search procedure we input the auxiliar url_array as a new column on our auxiliar data frame, and save a new Excel file. Ths file contains the article title, DOI and URL.\n",
    "\n",
    "For a new version of this search procedure we need to optimize the search by selecting the correct site. In our first attempet we got only the first result, however this sometimes obtains strage sites which will ruin the next steps.\n",
    "We can try to tackle this problem by obtaining more results and by a RegEx comparission, get the correct URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  DI  \\\n",
      "0     10.1590/2318-0889202032e190080   \n",
      "1     10.1590/1678-9865202032e190067   \n",
      "2     10.1590/2318-08892016002800001   \n",
      "3     10.1590/0103-37862015000300005   \n",
      "4     10.1590/2318-08892018000300002   \n",
      "..                               ...   \n",
      "248  10.1590/S0103-37862014000100009   \n",
      "249  10.1590/S0103-37862013000300007   \n",
      "250  10.1590/S0103-37862011000200002   \n",
      "251  10.1590/S0103-37862011000300005   \n",
      "252  10.1590/S0103-37862011000200005   \n",
      "\n",
      "                                                    TI  \\\n",
      "0    FRBR AND RDA BASED METADATA APPLICATIONS IN DI...   \n",
      "1    EYE TRACKING AND USABILITY IN DIGITAL INFORMAT...   \n",
      "2    METHODOLOGICAL CHOICES FOR RESEARCH IN INFORMA...   \n",
      "3    EDUCATIONAL POTENTIAL OF TOPIC MAPS AND LEARNI...   \n",
      "4    BUILDING TEACHING CLUSTERS BY RESEARCH INTERES...   \n",
      "..                                                 ...   \n",
      "248  INFORMATION SCIENCE: THEORETICAL-DISCIPLINARY ...   \n",
      "249       TWO-MODE SOCIAL NETWORKS: CONCEPTUAL ASPECTS   \n",
      "250  THE SEARCH FOR INFORMATION ON THE INTERNET: A ...   \n",
      "251  THE THEORETICAL APPROACH OF LENA VANIA RIBEIRO...   \n",
      "252  IMPROVEMENT OF STRATEGISTS AND NOVICE DECISION...   \n",
      "\n",
      "                                                   URL  \n",
      "0    https://www.scielo.br/j/tinf/a/jwdGWMncgQLpJFX...  \n",
      "1    https://www.scielo.br/j/tinf/a/bMKTg5NzQpZqcsk...  \n",
      "2    https://www.scielo.br/j/tinf/a/PBgJwdSD5phK5xx...  \n",
      "3    https://www.scielo.br/scielo.php?script=sci_ar...  \n",
      "4    https://www.scielo.br/j/tinf/a/cVxrmJRdz9pNDSr...  \n",
      "..                                                 ...  \n",
      "248  https://www.scielo.br/scielo.php?script=sci_ar...  \n",
      "249  https://www.scielo.br/j/cadsc/a/DFFvRydD6rt9Rv...  \n",
      "250  https://www.scielo.br/scielo.php?script=sci_ar...  \n",
      "251  https://www.scielo.br/scielo.php?script=sci_ar...  \n",
      "252  https://www.scielo.br/scielo.php?script=sci_ar...  \n",
      "\n",
      "[253 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print (raw_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_df = pd.read_excel('URL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DI</th>\n",
       "      <th>TI</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.1590/2313-0889201931e190033</td>\n",
       "      <td>A PANORAMA OF BRAZILIAN SCIENTIFIC PRODUCTION ...</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/gWy9yV67t3WHRmW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1590/S0103-37862012000200003</td>\n",
       "      <td>PUERTOTEX: A DATA MINING SOFTWARE BASED ON ONT...</td>\n",
       "      <td>http://www.scielo.br/scielo.php?pid=S0103-3786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1590/S0103-37862013000200001</td>\n",
       "      <td>THE SOCIAL DIMENSION OF MUSICAL GENRES: WHY CO...</td>\n",
       "      <td>http://www.scielo.br/scielo.php?pid=S0103-3786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1590/0103-37862015000200007</td>\n",
       "      <td>ENVIRONMENTAL SCANNING PROCESS IN THE TANNERIE...</td>\n",
       "      <td>http://www.scielo.br/scielo.php?pid=S0103-3786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.1590/0103-37862015000300001</td>\n",
       "      <td>NAME AMBIGUITY INFLUENCES CENTRALITY OF CO-AUT...</td>\n",
       "      <td>http://www.scielo.br/scielo.php?pid=S0103-3786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISSEMINATION OF INFORMATION WITHIN THE PUBLIC...</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/szjCKGNMf8SS8Cw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOCIAL INFORMATION</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/P9cL4pbw6D7yTtC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USING OPEN DATA TECHNOLOGY TO CONNECT LIBRARIE...</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/V37ZwhN3m9xgDzF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REPRESENTATIONS OF JOINT ECONOMIC VENTURES ON ...</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/TPrQVCGV6ctnNzd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLLECTION DEVELOPMENT: BIRTH OF CONTEMPORARY ...</td>\n",
       "      <td>https://www.scielo.br/j/tinf/a/PMK9FqgDj9rMs9W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                               DI  \\\n",
       "0             0   10.1590/2313-0889201931e190033   \n",
       "1             1  10.1590/S0103-37862012000200003   \n",
       "2             2  10.1590/S0103-37862013000200001   \n",
       "3             3   10.1590/0103-37862015000200007   \n",
       "4             4   10.1590/0103-37862015000300001   \n",
       "..          ...                              ...   \n",
       "248         248                              NaN   \n",
       "249         249                              NaN   \n",
       "250         250                              NaN   \n",
       "251         251                              NaN   \n",
       "252         252                              NaN   \n",
       "\n",
       "                                                    TI  \\\n",
       "0    A PANORAMA OF BRAZILIAN SCIENTIFIC PRODUCTION ...   \n",
       "1    PUERTOTEX: A DATA MINING SOFTWARE BASED ON ONT...   \n",
       "2    THE SOCIAL DIMENSION OF MUSICAL GENRES: WHY CO...   \n",
       "3    ENVIRONMENTAL SCANNING PROCESS IN THE TANNERIE...   \n",
       "4    NAME AMBIGUITY INFLUENCES CENTRALITY OF CO-AUT...   \n",
       "..                                                 ...   \n",
       "248  DISSEMINATION OF INFORMATION WITHIN THE PUBLIC...   \n",
       "249                                 SOCIAL INFORMATION   \n",
       "250  USING OPEN DATA TECHNOLOGY TO CONNECT LIBRARIE...   \n",
       "251  REPRESENTATIONS OF JOINT ECONOMIC VENTURES ON ...   \n",
       "252  COLLECTION DEVELOPMENT: BIRTH OF CONTEMPORARY ...   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://www.scielo.br/j/tinf/a/gWy9yV67t3WHRmW...  \n",
       "1    http://www.scielo.br/scielo.php?pid=S0103-3786...  \n",
       "2    http://www.scielo.br/scielo.php?pid=S0103-3786...  \n",
       "3    http://www.scielo.br/scielo.php?pid=S0103-3786...  \n",
       "4    http://www.scielo.br/scielo.php?pid=S0103-3786...  \n",
       "..                                                 ...  \n",
       "248  https://www.scielo.br/j/tinf/a/szjCKGNMf8SS8Cw...  \n",
       "249  https://www.scielo.br/j/tinf/a/P9cL4pbw6D7yTtC...  \n",
       "250  https://www.scielo.br/j/tinf/a/V37ZwhN3m9xgDzF...  \n",
       "251  https://www.scielo.br/j/tinf/a/TPrQVCGV6ctnNzd...  \n",
       "252  https://www.scielo.br/j/tinf/a/PMK9FqgDj9rMs9W...  \n",
       "\n",
       "[253 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the URLs we can proceed and scrap the Scielo site to extract the informations that we want. For this case we want to obtain the authors name, correct affiliations and the dates that the article was received, processed and published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.empty([Author_df['TI'].size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    TI  \\\n",
      "0    FRBR AND RDA BASED METADATA APPLICATIONS IN DI...   \n",
      "1    EYE TRACKING AND USABILITY IN DIGITAL INFORMAT...   \n",
      "2    METHODOLOGICAL CHOICES FOR RESEARCH IN INFORMA...   \n",
      "3    EDUCATIONAL POTENTIAL OF TOPIC MAPS AND LEARNI...   \n",
      "4    BUILDING TEACHING CLUSTERS BY RESEARCH INTERES...   \n",
      "..                                                 ...   \n",
      "248  INFORMATION SCIENCE: THEORETICAL-DISCIPLINARY ...   \n",
      "249       TWO-MODE SOCIAL NETWORKS: CONCEPTUAL ASPECTS   \n",
      "250  THE SEARCH FOR INFORMATION ON THE INTERNET: A ...   \n",
      "251  THE THEORETICAL APPROACH OF LENA VANIA RIBEIRO...   \n",
      "252  IMPROVEMENT OF STRATEGISTS AND NOVICE DECISION...   \n",
      "\n",
      "                                                Author  Article_Timeline  \n",
      "0    [Rogério MUGNAINI Universidade de São Paulo, E...     1.030119e-311  \n",
      "1    [Amed Leiva-Mederos, Sandor Domínguez-Velasco,...     1.030119e-311  \n",
      "2                                 [Rose Marie Santini]     1.030119e-311  \n",
      "3    [Marcia Loureiro PAULO Universidade Federal de...     1.030118e-311  \n",
      "4    [Rafael Garcia BARBASTEFANO Ministério da Educ...     1.030119e-311  \n",
      "..                                                 ...               ...  \n",
      "248                                       1.03012e-311     1.030119e-311  \n",
      "249                                       1.03012e-311     1.030119e-311  \n",
      "250                                       1.03012e-311     1.030119e-311  \n",
      "251                                       1.03012e-311     1.030119e-311  \n",
      "252                                       1.03012e-311     1.030119e-311  \n",
      "\n",
      "[253 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Author_df['Article_Timeline'] = array\n",
    "print ((Author_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat new columns with the information that we want to collect from the Scielo web site. Now we are ready to start the scraping procedure. We can first analyze the Scielo site to understand and select all the patterns within the HTML code where our information is located. To perform the scraping use the bs4 library importing the BeautifulSoup modulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n"
     ]
    }
   ],
   "source": [
    "## Scan each URL and get the author name & affiliation, article timeline\n",
    "# The first loop goes throug all the article's URLs and make \n",
    "for x in range (URL_df['URL'].size):\n",
    "    src = requests.get(URL_df['URL'].iloc[x])\n",
    "    document = BeautifulSoup(src.content)\n",
    "    Author = []\n",
    "    Article_Timeline = []\n",
    "    index = 0\n",
    "## We choose to work with the while loop and the try except routine as we do not know the numbers of the authors on the given\n",
    "#article neither if the article have the time line information.\n",
    "    while True:\n",
    "        try:\n",
    "            aux = document.find_all('div', class_ = 'tutors')[index].text.strip().split('\\n')[0]\n",
    "            Author.append(aux)\n",
    "            index = index + 1\n",
    "        except IndexError:\n",
    "            break\n",
    "    Author_df['Author'].iloc[x] = Author\n",
    "    try:\n",
    "        for j in (0,1):\n",
    "            timeline_aux = document.find_all('ul', class_ = 'articleTimeline')[j].text.strip()\n",
    "            Article_Timeline.append(timeline_aux)\n",
    "        Author_df['Article_Timeline'].iloc[x] = Article_Timeline\n",
    "    except IndexError:\n",
    "        Author_df['Article_Timeline'].iloc[x] = ''\n",
    "    print (x)\n",
    "    time.sleep(30)\n",
    "## It is also important to set a waiting time in order to avoid interruptions in our searching routine.\n",
    "# We still ned to work on the optimizing values for these waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Author_df.to_excel('Author_Timeline.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally save this final data frame to an Excel file containing the Authors' information and the articles' time line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ul class=\"articleTimeline\"><li>\n",
      "<strong>Publicación en esta colección</strong><br/>Jan-Apr 2018</li></ul>, <ul class=\"articleTimeline\">\n",
      "<li>\n",
      "<strong>Recibido</strong><br/>06 Mayo 2017</li>\n",
      "<li>\n",
      "<strong>Acepto</strong><br/>26 Jul 2017</li>\n",
      "</ul>]\n"
     ]
    }
   ],
   "source": [
    "## Some sites have only the abstracts and does not display the article timeline. On top of such sites there is a link to the\n",
    "#actual site containing the full article. So we get this link first and then access all the information from the site with\n",
    "#the full acrticle.\n",
    "URL = 'https://www.scielo.br/j/tinf/a/LHnv8vL7bN5GFcsmrb98qqM/abstract/?lang=en&format=html'\n",
    "src = requests.get(URL)\n",
    "document = BeautifulSoup(src.content)\n",
    "link = document.find_all('a', class_ = '_doi')[0].text.strip()\n",
    "src = requests.get(link)\n",
    "document2 = BeautifulSoup(src.content)\n",
    "article_timeline = document2.find_all('ul', class_ = 'articleTimeline')\n",
    "print (article_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Publication in this collection12\\xa0Dec\\xa02013\\n\\nDate of issueDec\\xa02013', 'Received17\\xa0Oct\\xa02012\\n\\nAccepted08\\xa0Mar\\xa02013\\n\\nReviewed24\\xa0Jan\\xa02013']\n"
     ]
    }
   ],
   "source": [
    "Article_Timeline = []\n",
    "for j in (0,1):\n",
    "    src = requests.get(URL_df['URL'].iloc[15])\n",
    "    document = BeautifulSoup(src.content)\n",
    "    Article_time_line = document.find_all('ul', class_ = 'articleTimeline')[j].text.strip()\n",
    "    Article_Timeline.append(Article_time_line)\n",
    "print (Article_Timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUGNAINI\n",
      "Rogério \n",
      "Universidade de São Paulo, Escola de Comunicações e Artes, Programa de Pós-Graduação em Ciência da Informação. Av. Professor Lúcio Martins Rodrigues, 443, Butantã, 05508-020, São Paulo, SP, Brasil.\n",
      "http://orcid.org/0000-0001-9334-3448\n",
      "DAMACENO\n",
      "Rafael Jeferson Pezzuto \n",
      "Universidade Federal do\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,2):\n",
    "    Author = document.find_all('div', class_ = 'tutors')[i].text.strip()\n",
    "    Last_Name = re.search(r'[A-Z]{3,}', Author)[0]\n",
    "    First_Name = aux = re.split(r'[A-Z]{3,}', Author)[0]\n",
    "    Aff = re.split(r'[A-Z]{3,}', Author)[1]\n",
    "    print (Last_Name)\n",
    "    print (First_Name)\n",
    "    print (Aff.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-6e2ffa318de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mArticle_time_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'articleTimeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mRecebido\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Recebido'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArticle_time_line\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mRevisado\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Revisado'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArticle_time_line\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mAceito\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Aceito'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArticle_time_line\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(Article_time_line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Article_time_line = document.find_all('ul', class_ = 'articleTimeline')[1].text.strip().split(' ')[0].split('\\n')\n",
    "Recebido = re.sub(r'Recebido', '', Article_time_line[0])\n",
    "Revisado = re.sub(r'Revisado', '', Article_time_line[2])\n",
    "Aceito = re.sub(r'Aceito', '', Article_time_line[4])\n",
    "#print(Article_time_line)\n",
    "print('Artigo recebido em:',Recebido)\n",
    "print('Artigo revisado em:',Revisado)\n",
    "print('Artigo aceito em:',Aceito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article_title = document.find_all('h2', class_ = 'article-title')[0].text.strip()\n",
    "\n",
    "print(Article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = requests.get(URL_df['URL'].iloc[0])\n",
    "document = BeautifulSoup(src.content)\n",
    "print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    Author = document.find_all('div', class_ = 'tutors')[0].text.strip().split(' ')\n",
    "    #Author_ = document.find_all('div', class_ = 'tutors')\n",
    "    Know_author = ['Natalia', 'Gallo', 'CERRAO']\n",
    "    res = [ ele for ele in Author ]\n",
    "    for a in Know_author:\n",
    "        if a in Author:\n",
    "            res.remove(a)\n",
    "    res_final = ' '.join(res)\n",
    "    print (res_final)\n",
    "    #print (Author_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
